{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "#import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import *\n",
    "import torch.utils.data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare initial parameters \n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "\n",
    "clip_size = 18\n",
    "nclips = 1\n",
    "step_size = 2\n",
    "is_val=True\n",
    "\n",
    "# declare transform (crop, mean, std) #\n",
    "\n",
    "transform = Compose([\n",
    "    CenterCrop(84),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406],\n",
    "              std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# root = './test_video/9223/%05d.jpg'%i     % path of frames\n",
    "\n",
    "# define function to load image from a path of image file \n",
    "def image_loader(image_path):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return image\n",
    "\n",
    "# define function to take file name of frame in a forder\n",
    "\n",
    "def get_frame_names(path):\n",
    "    frame_names = []\n",
    "    for ext in IMG_EXTENSIONS:\n",
    "        frame_names.extend(glob.glob(os.path.join(path, \"*\" + ext)))\n",
    "    frame_names = list(sorted(frame_names))\n",
    "    num_frames = len(frame_names)\n",
    "    # set number of necessary frames\n",
    "    if nclips > -1:\n",
    "        num_frames_necessary = clip_size * nclips * step_size\n",
    "    else:\n",
    "        num_frames_necessary = num_frames\n",
    "\n",
    "    # pick frames\n",
    "    offset = 0\n",
    "    if num_frames_necessary > num_frames:\n",
    "        # Pad last frame if video is shorter than necessary\n",
    "        frame_names += [frame_names[-1]] * \\\n",
    "            (num_frames_necessary - num_frames)\n",
    "    elif num_frames_necessary < num_frames:\n",
    "        # If there are more frames, then sample starting offset.\n",
    "        diff = (num_frames - num_frames_necessary)\n",
    "        # temporal augmentation\n",
    "        if not is_val:\n",
    "            offset = np.random.randint(0, diff)\n",
    "    frame_names = frame_names[offset:num_frames_necessary +\n",
    "                              offset:step_size]\n",
    "    return frame_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ListDataJpeg = namedtuple('ListDataJpeg', ['id', 'label', 'path'])\n",
    "# define function to read data from csv file\n",
    "class JpegDataset(object):\n",
    "\n",
    "    def __init__(self, csv_path_input, csv_path_labels, data_root):\n",
    "        self.csv_data = self.read_csv_input(csv_path_input, data_root)\n",
    "        self.classes = self.read_csv_labels(csv_path_labels)\n",
    "        self.classes_dict = self.get_two_way_dict(self.classes)\n",
    "\n",
    "    def read_csv_input(self, csv_path, data_root):\n",
    "        csv_data = []\n",
    "        with open(csv_path) as csvfile:\n",
    "            csv_reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in csv_reader:\n",
    "                item = ListDataJpeg(row[0],\n",
    "                                    row[1],\n",
    "                                    os.path.join(data_root, row[0])\n",
    "                                    )\n",
    "                csv_data.append(item)\n",
    "        return csv_data\n",
    "\n",
    "    def read_csv_labels(self, csv_path):\n",
    "        classes = []\n",
    "        with open(csv_path) as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            for row in csv_reader:\n",
    "                classes.append(row[0])\n",
    "        return classes\n",
    "\n",
    "    def get_two_way_dict(self, classes):\n",
    "        classes_dict = {}\n",
    "        for i, item in enumerate(classes):\n",
    "            classes_dict[item] = i\n",
    "            classes_dict[i] = item\n",
    "        return classes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "dataset_object = JpegDataset(data_root=\"/home/hoanganh/20bn-datasets/20bn-jester-v1/\",\n",
    "                     csv_path_input= \"csv_files1/jester-v7-test.csv\",\n",
    "                     csv_path_labels= \"csv_files/jester-v1-labels.csv\",)\n",
    "\n",
    "csv_data= dataset_object.csv_data\n",
    "print(len(csv_data))\n",
    "path_names = []\n",
    "target_idx = []\n",
    "for index in range(len(csv_data)):\n",
    "    item = csv_data[index]\n",
    "    path_names.append(item.path)\n",
    "    target_index = dataset_object.classes_dict[item.label]\n",
    "    target_idx.append(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hoanganh/20bn-datasets/20bn-jester-v1/100006', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100021', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100048', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100059', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100075', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100090', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100126', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100132', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100135', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100153', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100183', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100208', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100214', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100226', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100232', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100264', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10033', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100339', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100351', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100353', '/home/hoanganh/20bn-datasets/20bn-jester-v1/1004', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100420', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100438', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100443', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100444', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100445', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100460', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100462', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100480', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100482', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100495', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100502', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100510', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100533', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100550', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100554', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100580', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100583', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10061', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100653', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100691', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100714', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100718', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100724', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100729', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100763', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100789', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100793', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100800', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100821', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100839', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100871', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100872', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100890', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100930', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100939', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100947', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100961', '/home/hoanganh/20bn-datasets/20bn-jester-v1/100982', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101000', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101020', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101037', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101042', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101067', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10108', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101101', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101130', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101142', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101162', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101167', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10117', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101181', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101194', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101218', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10123', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101279', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101288', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101298', '/home/hoanganh/20bn-datasets/20bn-jester-v1/1013', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101308', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101339', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101346', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101370', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101382', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101384', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101399', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101404', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101407', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101419', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101425', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101432', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101436', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101450', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101453', '/home/hoanganh/20bn-datasets/20bn-jester-v1/10149', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101504', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101527', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101528', '/home/hoanganh/20bn-datasets/20bn-jester-v1/101532']\n",
      "[2, 3, 3, 2, 1, 6, 0, 6, 5, 6, 4, 6, 6, 3, 2, 6, 1, 3, 6, 5, 5, 0, 2, 4, 3, 0, 6, 3, 5, 2, 6, 0, 0, 2, 4, 5, 2, 6, 6, 4, 2, 5, 5, 0, 1, 4, 2, 3, 6, 4, 1, 3, 6, 6, 0, 6, 1, 4, 2, 4, 5, 4, 2, 0, 3, 0, 6, 1, 5, 0, 6, 3, 3, 1, 6, 0, 1, 6, 4, 1, 2, 0, 1, 3, 4, 1, 3, 6, 2, 6, 4, 4, 4, 6, 4, 1, 5, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "print(path_names)\n",
    "print(target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 3, 18, 84, 84)\n",
      "0.8612079620361328\n"
     ]
    }
   ],
   "source": [
    "### get image paths from an input forder\n",
    "train_dataset = []\n",
    "start = time.time()\n",
    "for path in path_names:\n",
    "\n",
    "    img_paths = get_frame_names(path)\n",
    "    ### get frames form img_paths \n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    for img_path in img_paths:\n",
    "        img = image_loader(img_path)\n",
    "        img = transform(img)\n",
    "        imgs.append(torch.unsqueeze(img, 0))\n",
    "\n",
    "    # print(imgs.shape)    \n",
    "    ## format data to torch\n",
    "\n",
    "    data = torch.cat(imgs)\n",
    "    data = data.permute(1, 0, 2, 3)\n",
    "    data = data.numpy()\n",
    "    train_dataset.append(data)\n",
    "    \n",
    "    # data = data.unsqueeze(0)\n",
    "\n",
    "train_dataset = np.array(train_dataset)\n",
    "print(train_dataset.shape)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
